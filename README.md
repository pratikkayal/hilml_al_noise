# Robust and Reproducible Active Learning with Noisy Labels: An Empirical Study on CIFAR-10

Active learning (AL) is a machine learning (ML) technique that selects the most informative unlabeled data for annotation, aiming to reduce the labeling cost and improve the model performance. However, in many real-world scenarios, the labels obtained from human annotators may be noisy and unreliable, which can degrade the effectiveness of AL methods. In this paper, we investigate the performance of seven AL methods: Random, MinMargin, MaxEntropy, DBAL, BALD, Coreset, and VAAL, under various levels of label noise on the CIFAR-10 dataset. We use neural networks as the base learners and measure the test accuracy of each method after querying a fixed number of labels. Our results show that MinMargin and BALD achieve the highest accuracy across all noise levels, while Random and MaxEntropy are the most robust to high noise. We also observe that the performance of all methods drops significantly when the noise level reaches 1.0, indicating that AL is sensitive to label quality and requires reliable annotation sources. We discuss the implications of our findings for the field of AL and machine learning in general, and provide some insights and suggestions on how to choose and design AL methods for noisy data. We also share our code and human-annotated noisy labels for CIFAR-10 to facilitate further research on this topic.

## Introduction
Machine learning (ML) models have achieved remarkable success in various domains, such as computer vision, natural language processing, and speech recognition. However, these models often require a large amount of labeled data to achieve high performance, which can be costly and time-consuming to obtain. Moreover, the quality of the labels may not be reliable, as they can be affected by human errors, biases, or malicious attacks. Therefore, it is desirable to develop ML methods that can learn effectively from limited and noisy data.

Active learning (AL) is a ML paradigm that aims to reduce the labeling cost and improve the model performance by intelligently selecting the most informative samples from a large pool of unlabeled data for annotation. In AL, a model trained with a small amount of labeled seed data is used to parse through the unlabeled data to select the subset that should be sent to an oracle (annotator). To select such a subset, AL methods rely on exploiting the learned latent space, model uncertainty, or other heuristics. AL has been widely applied to various tasks, such as image classification, text categorization, and sentiment analysis.

However, AL methods face several challenges and limitations in practice. First, the results reported for AL methods are often inconsistent and not reproducible across different studies, due to the variations in experimental settings, such as data splits, model architectures, evaluation metrics, and random seeds. Second, the results reported for AL methods are often based on simplistic and idealized assumptions, such as the availability of a perfect oracle, the absence of label noise, and the fixed hyper-parameters. Third, the results reported for AL methods are often not robust and generalizable to different scenarios, such as the presence of noisy or adversarial labels, the changes in data distribution, and the variations in query budget.

In this paper, we address these challenges and limitations by conducting a comprehensive and systematic evaluation of different AL methods on the CIFAR-10 dataset with various levels of synthetic label noise. We use neural networks as the base learners and evaluate the AL methods based on their accuracy, robustness, and sample efficiency. We also investigate the impact of noise correction and regularization techniques on the AL performance. Our main contributions are as follows:
